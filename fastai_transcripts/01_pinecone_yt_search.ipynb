{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d07180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import txtai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64557996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.0.0'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtai.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1dCsisKaCPx",
   "metadata": {},
   "source": [
    "Install prerequisite packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb8e8fd",
   "metadata": {},
   "source": [
    "# YouTube Indexing and Queries\n",
    "\n",
    "In this notebook we will work through an example of indexing and querying the YouTube video transcriptions data. We start by loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b97f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43af099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 23:14:56,487 [WARNING] _create_builder_config: Using custom data configuration data-02fffa30d1cc87c7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/data to /home/matu/.cache/huggingface/datasets/json/data-02fffa30d1cc87c7/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ccff12e8dd423c94453a95d1fe126b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdbe449d47c440268f38da2be7f8b993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/matu/.cache/huggingface/datasets/json/data-02fffa30d1cc87c7/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'title', 'text', 'start_second', 'end_second'],\n",
       "    num_rows: 8806\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytt = load_dataset(\n",
    "    \"./data\",\n",
    "    split=\"train\",\n",
    ")\n",
    "ytt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db35007",
   "metadata": {},
   "source": [
    "Each sample includes video-level information (ID, title, url and thumbnail) and snippet-level information (text, start_second, end_second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e73d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://www.youtube.com/watch?v=8SF_h3xF3cE&t=0s', 'title': 'Lesson 1: Practical Deep Learning for Coders 2022', 'text': \"  Welcome to practical deep learning for coders lesson one.  This is version five of this course.  And it's the first do one we've done in two years.  So we've got a lot of cool things to cover.  It's amazing how much has changed.  Here is a XKCD from the end of 2015.  Who here is saying XKCD comics before?  Pretty much everybody, not surprising.  So the basic joke here is I'll let you read it,  and then I'll come back to it.  So it can be hard to tell what's easy and what's nearly impossible.\", 'start_second': 0.0, 'end_second': 58.0}\n"
     ]
    }
   ],
   "source": [
    "for x in ytt:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38004c9",
   "metadata": {},
   "source": [
    "# generate the TXTAI database with vectors\n",
    "\n",
    "The next step is indexing this dataset in Pinecone, for this we need a sentence transformer model (to encode the text into embeddings), and a Pinecone index.\n",
    "\n",
    "We will initialize the sentence transformer first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a6511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai.embeddings import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ffd304",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Embeddings(\n",
    "                        {\n",
    "                            \"path\": \"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    "                            \"content\": True,\n",
    "                            \"objects\": True\n",
    "                        }\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d53d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.index([(uid, data, None) for uid, data in enumerate(ytt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f498976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5707</td>\n",
       "      <td>Let's more discuss in the next lesson, we'll talk a lot about neural net architecture details.  But the details we'll focus on are what happens to the inputs at the very first stage and  what happens to the outputs at the very last stage.  We'll talk a bit about what happens in the middle, but a lot less.  And the reason why is it's a thing that you put into the inputs that's going to change  for every single data set you do.  And what do you want to happen to the outputs that are going to...</td>\n",
       "      <td>0.566333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3331</td>\n",
       "      <td>It's modifying things together and adding them up.  So there'd be one more step to do to make this a layer of a neural network which is  if this had any negatives, we'd place them with yours.  That's why matrix multiplication is the critical foundation or mathematical operation in basically  all of deep learning.  So the GPUs that we use, the thing that they are good at is this, matrix multiplication.  They have special cores called tensor cores which can basically only do one thing which ...</td>\n",
       "      <td>0.566635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>8424</td>\n",
       "      <td>And what you see in the in the next column is a version of the image where the horizontal lines being recognized.  And another one where the vertical lines are being recognized. And if you think back to that Xyla and Fergus paper that talked about what the layers of a neural net does, this is an absolutely an example of something that we know that the first layer of a neural network  tends to learn how to do.  Now, how did I do this? I did this using something called a convolution.  And so...</td>\n",
       "      <td>0.566640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1316</td>\n",
       "      <td>And then it takes those as inputs to a next layer.  It does the same thing.  It multiplies them a bunch of times and adds them up.  And it does that a few times.  And that's called a neural network.  Now, the model, therefore, is not  going to do anything useful.  And this leads weights to very carefully, it shows it.  And so the way it works is that we actually  start out with these weights as being random.  So initially, this thing doesn't do anything useful at all.</td>\n",
       "      <td>0.566887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2750</td>\n",
       "      <td>We could have a look at zero dot model dot stages dot zero dot blocks dot one dot MLP  dot F C one and parameters and other big bunch of numbers.  So what's going on here?  What are these numbers and where it is that they come from and how come these numbers  can figure out whether something is a basset hound or not?  Okay.  So to answer that question, we're going to have a look at a Kaggle notebook.  How does a neural network really work?  I've got a local version of it here, which I'm go...</td>\n",
       "      <td>0.568176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2760</td>\n",
       "      <td>So we start out with a very, very flexible, in fact, an infinitely flexible as we discussed  function, a neural network.  And we get it to do a particular thing, which is to recognize the patterns in the data examples  we give it.  So let's do a much simpler example than a neural network.  Let's do a quadratic.  So let's create a function F, which is 3x squared plus 2x plus 1.  Okay.  So it's a quadratic with coefficient 3, 2, and 1.  So we can plot that function F and give it a title.</td>\n",
       "      <td>0.568182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>8410</td>\n",
       "      <td>And I mentioned that there are other things that can go in the middle as well, but we haven't really talked about what those other things are.  So I thought we might look at one of the most important and interesting version of things that can go in the middle.  But what you'll see is it turns out it's actually just another kind of matrix modification, which might not be obvious at first, but I'll explain.  We're going to look at something called a convolution, and convolutions are at the h...</td>\n",
       "      <td>0.568650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1322</td>\n",
       "      <td>going to do anything useful.  And this leads weights to very carefully, it shows it.  And so the way it works is that we actually  start out with these weights as being random.  So initially, this thing doesn't do anything useful at all.  So what we do, the way Arthur Samuel described it back  in the late 50s, the inventor of machine learning,  is he said, OK, let's take the inputs and the weights,  put them through our model.  He wasn't talking particularly about neural networks.  He's ju...</td>\n",
       "      <td>0.569121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4195</td>\n",
       "      <td>I tend to use this one because it's less typing.  So you can see now we've got these concatenated rows.  So head is the first few rows.  So we've now got some documents to do an LP with.  Now, the problem is, as you know from the last lesson,  neural networks work with numbers.  We've got to take some numbers and we're going to multiply them by matrices.  We're going to replace the negatives with zeros,  net them up, and we're going to do that a few times.  That's our neural network.</td>\n",
       "      <td>0.569592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1323</td>\n",
       "      <td>And this leads weights to very carefully, it shows it.  And so the way it works is that we actually  start out with these weights as being random.  So initially, this thing doesn't do anything useful at all.  So what we do, the way Arthur Samuel described it back  in the late 50s, the inventor of machine learning,  is he said, OK, let's take the inputs and the weights,  put them through our model.  He wasn't talking particularly about neural networks.  He's just like, whatever model you li...</td>\n",
       "      <td>0.570379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2754</td>\n",
       "      <td>can figure out whether something is a basset hound or not?  Okay.  So to answer that question, we're going to have a look at a Kaggle notebook.  How does a neural network really work?  I've got a local version of it here, which I'm going to take you through.  And the basic idea is machine learning models are things that fit functions to data.  So we start out with a very, very flexible, in fact, an infinitely flexible as we discussed  function, a neural network.  And we get it to do a part...</td>\n",
       "      <td>0.570419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5897</td>\n",
       "      <td>small test set. I don't know if this is necessarily better or worse than the linear model, but  it's certainly fine. And I think that's pretty cool that we were able to build a neural net  from scratch. That's doing pretty well. But I hear that all the cool kids nowadays are  doing deep learning, not just neural nets. So we better make this deep learning. So this  one only has one hidden layer. So let's create one with n hidden layers. So for example,  let's say we want two hidden layers, ...</td>\n",
       "      <td>0.572909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4196</td>\n",
       "      <td>So you can see now we've got these concatenated rows.  So head is the first few rows.  So we've now got some documents to do an LP with.  Now, the problem is, as you know from the last lesson,  neural networks work with numbers.  We've got to take some numbers and we're going to multiply them by matrices.  We're going to replace the negatives with zeros,  net them up, and we're going to do that a few times.  That's our neural network.  Now with some little wrinkles, but that's the basic idea.</td>\n",
       "      <td>0.575390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5807</td>\n",
       "      <td>neural net in a moment.  And so that means rather than treat this as a matrix times a vector, I want to treat this  as a matrix times a matrix because we're about to add some more columns of coefficients.  So we're going to change in a cof's so that rather than creating an n cof vector, we're  going to create an n cof by one matrix.  So in math, we would probably call that a column vector.  But I think that's a kind of a dumb name in some ways because it's a matrix, right?</td>\n",
       "      <td>0.576456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1300</td>\n",
       "      <td>And we don't just have inputs now.  We now also have weights, which are also called parameters.  And the key thing is this.  The model is not any more a bunch of conditionals  and loops and things.  It's a mathematical function.  In the case of a neural network, it's  a mathematical function that takes the inputs,  multiplies them together by the weights, by one set of weights,  and adds them up.  And then it does that again for a second set of weights  and adds them up.</td>\n",
       "      <td>0.577518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>8480</td>\n",
       "      <td>So the size of this is called its kernel size. This is a three by three kernel for this convolution.  So then, because this is deep learning, we just repeat the, we just repeat these steps again and again and again.  So this is this layer I'm calling conv one, it's the first convolutional layer.  So con two, it's going to be a little bit different because on con one, we only had a single channel input. It's just black and white or, you know, yeah, black and white, grayscale one channel.  B...</td>\n",
       "      <td>0.578256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2753</td>\n",
       "      <td>What are these numbers and where it is that they come from and how come these numbers  can figure out whether something is a basset hound or not?  Okay.  So to answer that question, we're going to have a look at a Kaggle notebook.  How does a neural network really work?  I've got a local version of it here, which I'm going to take you through.  And the basic idea is machine learning models are things that fit functions to data.  So we start out with a very, very flexible, in fact, an infin...</td>\n",
       "      <td>0.578541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1303</td>\n",
       "      <td>The model is not any more a bunch of conditionals  and loops and things.  It's a mathematical function.  In the case of a neural network, it's  a mathematical function that takes the inputs,  multiplies them together by the weights, by one set of weights,  and adds them up.  And then it does that again for a second set of weights  and adds them up.  It does it again for a third set of weights and adds them up,  and so forth.  It then takes all the negative numbers</td>\n",
       "      <td>0.578821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8479</td>\n",
       "      <td>Right? You could have, we could just have easily done five by five, and then we'd have a five by five matrix of coefficients, or whatever, whatever size you like.  So the size of this is called its kernel size. This is a three by three kernel for this convolution.  So then, because this is deep learning, we just repeat the, we just repeat these steps again and again and again.  So this is this layer I'm calling conv one, it's the first convolutional layer.  So con two, it's going to be a l...</td>\n",
       "      <td>0.579774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8581</td>\n",
       "      <td>I could say, okay, we've got all these different things a BCD FTH J. Let's put them all into a single vector.  And then let's create a single matrix that has alpha alpha alpha alpha beta beta beta beta etc.  And then if we do this matrix multiplied by this vector, we get this with these gray zeros in the appropriate places, which gives us this, which is the same as this.  And so this shows that a convolution is actually a special kind of matrix modification. It's a matrix modification wher...</td>\n",
       "      <td>0.580621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5842</td>\n",
       "      <td>ahead and create a neural network.  So with a neural network, remember back to the Excel days, notice here is the same thing,  right?  A column vector, but we didn't create a column vector, we actually created a matrix with  kind of two sets of coefficients.  So when we did our matrix model play, every row gave us two sets of outputs, which we  then chucked through value, right, which remember we just used an if statement, and we added  them together.  So our cof's now, to make a proper ne...</td>\n",
       "      <td>0.580956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8580</td>\n",
       "      <td>But here's another way of thinking about it.  I could say, okay, we've got all these different things a BCD FTH J. Let's put them all into a single vector.  And then let's create a single matrix that has alpha alpha alpha alpha beta beta beta beta etc.  And then if we do this matrix multiplied by this vector, we get this with these gray zeros in the appropriate places, which gives us this, which is the same as this.  And so this shows that a convolution is actually a special kind of matrix...</td>\n",
       "      <td>0.583673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8449</td>\n",
       "      <td>But it's doing it this way where it's kind of taking advantage of the geometry of the situation that the things that are close to each other.  Being multiplied by this consistent group of the same nine weights each time.  Because there's actually 28 by 28 numbers here, right, which I think is 768.  That plus enough, 784.  But we don't want we don't we don't have 784 parameters really have nine parameters.  And so this is called a convolution.  So a convolution is where you basically slide ...</td>\n",
       "      <td>0.585658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1326</td>\n",
       "      <td>So initially, this thing doesn't do anything useful at all.  So what we do, the way Arthur Samuel described it back  in the late 50s, the inventor of machine learning,  is he said, OK, let's take the inputs and the weights,  put them through our model.  He wasn't talking particularly about neural networks.  He's just like, whatever model you like.  Get the results.  And then let's decide how good they are.  So if, for example, we're trying to decide,  is this a picture of a bird?  And the ...</td>\n",
       "      <td>0.586377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2751</td>\n",
       "      <td>dot F C one and parameters and other big bunch of numbers.  So what's going on here?  What are these numbers and where it is that they come from and how come these numbers  can figure out whether something is a basset hound or not?  Okay.  So to answer that question, we're going to have a look at a Kaggle notebook.  How does a neural network really work?  I've got a local version of it here, which I'm going to take you through.  And the basic idea is machine learning models are things that...</td>\n",
       "      <td>0.588155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2752</td>\n",
       "      <td>So what's going on here?  What are these numbers and where it is that they come from and how come these numbers  can figure out whether something is a basset hound or not?  Okay.  So to answer that question, we're going to have a look at a Kaggle notebook.  How does a neural network really work?  I've got a local version of it here, which I'm going to take you through.  And the basic idea is machine learning models are things that fit functions to data.  So we start out with a very, very f...</td>\n",
       "      <td>0.588817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8478</td>\n",
       "      <td>And they don't have to be three by three.  Right? You could have, we could just have easily done five by five, and then we'd have a five by five matrix of coefficients, or whatever, whatever size you like.  So the size of this is called its kernel size. This is a three by three kernel for this convolution.  So then, because this is deep learning, we just repeat the, we just repeat these steps again and again and again.  So this is this layer I'm calling conv one, it's the first convolution...</td>\n",
       "      <td>0.590231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3876</td>\n",
       "      <td>And layer one had sets of weights that found diagonal edges.  And here are some examples of bits of photos  that successfully matched with and opposite diagonal edges.  And kind of color gradients.  And here's some examples of bits of pictures that matched.  And then layer two combined, those.  And now you know how those were combined, right?  These were rectified linear units that were added together.  And then sets of those rectified linear units,  the outputs of those, they're called ac...</td>\n",
       "      <td>0.590644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>438</td>\n",
       "      <td>As I say, it's very successful.  But it's not something that I could create for you  in a minute at the start of a course.  The difference with neural networks  is neural networks don't require us to build these features.  They build them for us.  And so what actually happened was, in I think it was 2015,  Matt Zyla and Rob Fergus took a trained neural network  and they looked inside it to see what it had learned.  So we don't give it features, we ask it to learn features.</td>\n",
       "      <td>0.590785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1299</td>\n",
       "      <td>called a model.  And we don't just have inputs now.  We now also have weights, which are also called parameters.  And the key thing is this.  The model is not any more a bunch of conditionals  and loops and things.  It's a mathematical function.  In the case of a neural network, it's  a mathematical function that takes the inputs,  multiplies them together by the weights, by one set of weights,  and adds them up.  And then it does that again for a second set of weights  and adds them up.</td>\n",
       "      <td>0.591769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>439</td>\n",
       "      <td>But it's not something that I could create for you  in a minute at the start of a course.  The difference with neural networks  is neural networks don't require us to build these features.  They build them for us.  And so what actually happened was, in I think it was 2015,  Matt Zyla and Rob Fergus took a trained neural network  and they looked inside it to see what it had learned.  So we don't give it features, we ask it to learn features.  So when Zyla and Fergus looked inside a neural n...</td>\n",
       "      <td>0.592611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2759</td>\n",
       "      <td>And the basic idea is machine learning models are things that fit functions to data.  So we start out with a very, very flexible, in fact, an infinitely flexible as we discussed  function, a neural network.  And we get it to do a particular thing, which is to recognize the patterns in the data examples  we give it.  So let's do a much simpler example than a neural network.  Let's do a quadratic.  So let's create a function F, which is 3x squared plus 2x plus 1.  Okay.  So it's a quadratic ...</td>\n",
       "      <td>0.594546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1320</td>\n",
       "      <td>And that's called a neural network.  Now, the model, therefore, is not  going to do anything useful.  And this leads weights to very carefully, it shows it.  And so the way it works is that we actually  start out with these weights as being random.  So initially, this thing doesn't do anything useful at all.  So what we do, the way Arthur Samuel described it back  in the late 50s, the inventor of machine learning,  is he said, OK, let's take the inputs and the weights,  put them through ou...</td>\n",
       "      <td>0.598521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8707</td>\n",
       "      <td>The foundations haven't changed. And it's not that different in fact to the convolutional neural network that Jan LeCun used on MNIST back in 1996.  It's, you know, the basic ideas I've described are forever, you know, the way the inputs work and the sandwiches of matrix,  and the model applies and activation functions and the stuff you do to the final layer, you know, everything else is tweaks.  And the more you learn about those basic ideas, the more you'll recognize those tweaks as simp...</td>\n",
       "      <td>0.601152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1309</td>\n",
       "      <td>and adds them up.  And then it does that again for a second set of weights  and adds them up.  It does it again for a third set of weights and adds them up,  and so forth.  It then takes all the negative numbers  and replaces them with zeros.  And then it takes those as inputs to a next layer.  It does the same thing.  It multiplies them a bunch of times and adds them up.  And it does that a few times.  And that's called a neural network.</td>\n",
       "      <td>0.601690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5843</td>\n",
       "      <td>So with a neural network, remember back to the Excel days, notice here is the same thing,  right?  A column vector, but we didn't create a column vector, we actually created a matrix with  kind of two sets of coefficients.  So when we did our matrix model play, every row gave us two sets of outputs, which we  then chucked through value, right, which remember we just used an if statement, and we added  them together.  So our cof's now, to make a proper neural net, we need one set of cof's h...</td>\n",
       "      <td>0.605886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8477</td>\n",
       "      <td>So you can think of a convolution as being a sliding window of little mini dot products of these little three by three matrices.  And they don't have to be three by three.  Right? You could have, we could just have easily done five by five, and then we'd have a five by five matrix of coefficients, or whatever, whatever size you like.  So the size of this is called its kernel size. This is a three by three kernel for this convolution.  So then, because this is deep learning, we just repeat ...</td>\n",
       "      <td>0.610260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1310</td>\n",
       "      <td>And then it does that again for a second set of weights  and adds them up.  It does it again for a third set of weights and adds them up,  and so forth.  It then takes all the negative numbers  and replaces them with zeros.  And then it takes those as inputs to a next layer.  It does the same thing.  It multiplies them a bunch of times and adds them up.  And it does that a few times.  And that's called a neural network.  Now, the model, therefore, is not</td>\n",
       "      <td>0.613787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1302</td>\n",
       "      <td>And the key thing is this.  The model is not any more a bunch of conditionals  and loops and things.  It's a mathematical function.  In the case of a neural network, it's  a mathematical function that takes the inputs,  multiplies them together by the weights, by one set of weights,  and adds them up.  And then it does that again for a second set of weights  and adds them up.  It does it again for a third set of weights and adds them up,  and so forth.</td>\n",
       "      <td>0.614021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1305</td>\n",
       "      <td>It's a mathematical function.  In the case of a neural network, it's  a mathematical function that takes the inputs,  multiplies them together by the weights, by one set of weights,  and adds them up.  And then it does that again for a second set of weights  and adds them up.  It does it again for a third set of weights and adds them up,  and so forth.  It then takes all the negative numbers  and replaces them with zeros.  And then it takes those as inputs to a next layer.</td>\n",
       "      <td>0.614602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2755</td>\n",
       "      <td>Okay.  So to answer that question, we're going to have a look at a Kaggle notebook.  How does a neural network really work?  I've got a local version of it here, which I'm going to take you through.  And the basic idea is machine learning models are things that fit functions to data.  So we start out with a very, very flexible, in fact, an infinitely flexible as we discussed  function, a neural network.  And we get it to do a particular thing, which is to recognize the patterns in the data...</td>\n",
       "      <td>0.622364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2758</td>\n",
       "      <td>I've got a local version of it here, which I'm going to take you through.  And the basic idea is machine learning models are things that fit functions to data.  So we start out with a very, very flexible, in fact, an infinitely flexible as we discussed  function, a neural network.  And we get it to do a particular thing, which is to recognize the patterns in the data examples  we give it.  So let's do a much simpler example than a neural network.  Let's do a quadratic.  So let's create a f...</td>\n",
       "      <td>0.623059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1304</td>\n",
       "      <td>and loops and things.  It's a mathematical function.  In the case of a neural network, it's  a mathematical function that takes the inputs,  multiplies them together by the weights, by one set of weights,  and adds them up.  And then it does that again for a second set of weights  and adds them up.  It does it again for a third set of weights and adds them up,  and so forth.  It then takes all the negative numbers  and replaces them with zeros.  And then it takes those as inputs to a next ...</td>\n",
       "      <td>0.628380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2756</td>\n",
       "      <td>So to answer that question, we're going to have a look at a Kaggle notebook.  How does a neural network really work?  I've got a local version of it here, which I'm going to take you through.  And the basic idea is machine learning models are things that fit functions to data.  So we start out with a very, very flexible, in fact, an infinitely flexible as we discussed  function, a neural network.  And we get it to do a particular thing, which is to recognize the patterns in the data examples</td>\n",
       "      <td>0.637606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2757</td>\n",
       "      <td>How does a neural network really work?  I've got a local version of it here, which I'm going to take you through.  And the basic idea is machine learning models are things that fit functions to data.  So we start out with a very, very flexible, in fact, an infinitely flexible as we discussed  function, a neural network.  And we get it to do a particular thing, which is to recognize the patterns in the data examples  we give it.  So let's do a much simpler example than a neural network.</td>\n",
       "      <td>0.638342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1306</td>\n",
       "      <td>In the case of a neural network, it's  a mathematical function that takes the inputs,  multiplies them together by the weights, by one set of weights,  and adds them up.  And then it does that again for a second set of weights  and adds them up.  It does it again for a third set of weights and adds them up,  and so forth.  It then takes all the negative numbers  and replaces them with zeros.  And then it takes those as inputs to a next layer.  It does the same thing.</td>\n",
       "      <td>0.639722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8411</td>\n",
       "      <td>So I thought we might look at one of the most important and interesting version of things that can go in the middle.  But what you'll see is it turns out it's actually just another kind of matrix modification, which might not be obvious at first, but I'll explain.  We're going to look at something called a convolution, and convolutions are at the heart of a convolutional neural network.  So the first thing to realize is a convolutional neural network is very, very, very similar to the neur...</td>\n",
       "      <td>0.654447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8412</td>\n",
       "      <td>But what you'll see is it turns out it's actually just another kind of matrix modification, which might not be obvious at first, but I'll explain.  We're going to look at something called a convolution, and convolutions are at the heart of a convolutional neural network.  So the first thing to realize is a convolutional neural network is very, very, very similar to the neural networks we've seen so far. It's got imports. It's got things that are a lot like are actually are a form of matrix...</td>\n",
       "      <td>0.710898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8414</td>\n",
       "      <td>So the first thing to realize is a convolutional neural network is very, very, very similar to the neural networks we've seen so far. It's got imports. It's got things that are a lot like are actually are a form of matrix multiplication sandwich with activation functions, which can be rectified linear.  But there's a particular thing, which makes them very useful for computer vision. And I'm going to show you using this Excel spreadsheet that's in our repo called conv example.  And we're g...</td>\n",
       "      <td>0.713412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8413</td>\n",
       "      <td>We're going to look at something called a convolution, and convolutions are at the heart of a convolutional neural network.  So the first thing to realize is a convolutional neural network is very, very, very similar to the neural networks we've seen so far. It's got imports. It's got things that are a lot like are actually are a form of matrix multiplication sandwich with activation functions, which can be rectified linear.  But there's a particular thing, which makes them very useful for...</td>\n",
       "      <td>0.741815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "49  5707   \n",
       "48  3331   \n",
       "47  8424   \n",
       "46  1316   \n",
       "45  2750   \n",
       "44  2760   \n",
       "43  8410   \n",
       "42  1322   \n",
       "41  4195   \n",
       "40  1323   \n",
       "39  2754   \n",
       "38  5897   \n",
       "37  4196   \n",
       "36  5807   \n",
       "35  1300   \n",
       "34  8480   \n",
       "33  2753   \n",
       "32  1303   \n",
       "31  8479   \n",
       "30  8581   \n",
       "29  5842   \n",
       "28  8580   \n",
       "27  8449   \n",
       "26  1326   \n",
       "25  2751   \n",
       "24  2752   \n",
       "23  8478   \n",
       "22  3876   \n",
       "21   438   \n",
       "20  1299   \n",
       "19   439   \n",
       "18  2759   \n",
       "17  1320   \n",
       "16  8707   \n",
       "15  1309   \n",
       "14  5843   \n",
       "13  8477   \n",
       "12  1310   \n",
       "11  1302   \n",
       "10  1305   \n",
       "9   2755   \n",
       "8   2758   \n",
       "7   1304   \n",
       "6   2756   \n",
       "5   2757   \n",
       "4   1306   \n",
       "3   8411   \n",
       "2   8412   \n",
       "1   8414   \n",
       "0   8413   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text  \\\n",
       "49    Let's more discuss in the next lesson, we'll talk a lot about neural net architecture details.  But the details we'll focus on are what happens to the inputs at the very first stage and  what happens to the outputs at the very last stage.  We'll talk a bit about what happens in the middle, but a lot less.  And the reason why is it's a thing that you put into the inputs that's going to change  for every single data set you do.  And what do you want to happen to the outputs that are going to...   \n",
       "48    It's modifying things together and adding them up.  So there'd be one more step to do to make this a layer of a neural network which is  if this had any negatives, we'd place them with yours.  That's why matrix multiplication is the critical foundation or mathematical operation in basically  all of deep learning.  So the GPUs that we use, the thing that they are good at is this, matrix multiplication.  They have special cores called tensor cores which can basically only do one thing which ...   \n",
       "47    And what you see in the in the next column is a version of the image where the horizontal lines being recognized.  And another one where the vertical lines are being recognized. And if you think back to that Xyla and Fergus paper that talked about what the layers of a neural net does, this is an absolutely an example of something that we know that the first layer of a neural network  tends to learn how to do.  Now, how did I do this? I did this using something called a convolution.  And so...   \n",
       "46                             And then it takes those as inputs to a next layer.  It does the same thing.  It multiplies them a bunch of times and adds them up.  And it does that a few times.  And that's called a neural network.  Now, the model, therefore, is not  going to do anything useful.  And this leads weights to very carefully, it shows it.  And so the way it works is that we actually  start out with these weights as being random.  So initially, this thing doesn't do anything useful at all.   \n",
       "45    We could have a look at zero dot model dot stages dot zero dot blocks dot one dot MLP  dot F C one and parameters and other big bunch of numbers.  So what's going on here?  What are these numbers and where it is that they come from and how come these numbers  can figure out whether something is a basset hound or not?  Okay.  So to answer that question, we're going to have a look at a Kaggle notebook.  How does a neural network really work?  I've got a local version of it here, which I'm go...   \n",
       "44           So we start out with a very, very flexible, in fact, an infinitely flexible as we discussed  function, a neural network.  And we get it to do a particular thing, which is to recognize the patterns in the data examples  we give it.  So let's do a much simpler example than a neural network.  Let's do a quadratic.  So let's create a function F, which is 3x squared plus 2x plus 1.  Okay.  So it's a quadratic with coefficient 3, 2, and 1.  So we can plot that function F and give it a title.   \n",
       "43    And I mentioned that there are other things that can go in the middle as well, but we haven't really talked about what those other things are.  So I thought we might look at one of the most important and interesting version of things that can go in the middle.  But what you'll see is it turns out it's actually just another kind of matrix modification, which might not be obvious at first, but I'll explain.  We're going to look at something called a convolution, and convolutions are at the h...   \n",
       "42    going to do anything useful.  And this leads weights to very carefully, it shows it.  And so the way it works is that we actually  start out with these weights as being random.  So initially, this thing doesn't do anything useful at all.  So what we do, the way Arthur Samuel described it back  in the late 50s, the inventor of machine learning,  is he said, OK, let's take the inputs and the weights,  put them through our model.  He wasn't talking particularly about neural networks.  He's ju...   \n",
       "41             I tend to use this one because it's less typing.  So you can see now we've got these concatenated rows.  So head is the first few rows.  So we've now got some documents to do an LP with.  Now, the problem is, as you know from the last lesson,  neural networks work with numbers.  We've got to take some numbers and we're going to multiply them by matrices.  We're going to replace the negatives with zeros,  net them up, and we're going to do that a few times.  That's our neural network.   \n",
       "40    And this leads weights to very carefully, it shows it.  And so the way it works is that we actually  start out with these weights as being random.  So initially, this thing doesn't do anything useful at all.  So what we do, the way Arthur Samuel described it back  in the late 50s, the inventor of machine learning,  is he said, OK, let's take the inputs and the weights,  put them through our model.  He wasn't talking particularly about neural networks.  He's just like, whatever model you li...   \n",
       "39    can figure out whether something is a basset hound or not?  Okay.  So to answer that question, we're going to have a look at a Kaggle notebook.  How does a neural network really work?  I've got a local version of it here, which I'm going to take you through.  And the basic idea is machine learning models are things that fit functions to data.  So we start out with a very, very flexible, in fact, an infinitely flexible as we discussed  function, a neural network.  And we get it to do a part...   \n",
       "38    small test set. I don't know if this is necessarily better or worse than the linear model, but  it's certainly fine. And I think that's pretty cool that we were able to build a neural net  from scratch. That's doing pretty well. But I hear that all the cool kids nowadays are  doing deep learning, not just neural nets. So we better make this deep learning. So this  one only has one hidden layer. So let's create one with n hidden layers. So for example,  let's say we want two hidden layers, ...   \n",
       "37    So you can see now we've got these concatenated rows.  So head is the first few rows.  So we've now got some documents to do an LP with.  Now, the problem is, as you know from the last lesson,  neural networks work with numbers.  We've got to take some numbers and we're going to multiply them by matrices.  We're going to replace the negatives with zeros,  net them up, and we're going to do that a few times.  That's our neural network.  Now with some little wrinkles, but that's the basic idea.   \n",
       "36                        neural net in a moment.  And so that means rather than treat this as a matrix times a vector, I want to treat this  as a matrix times a matrix because we're about to add some more columns of coefficients.  So we're going to change in a cof's so that rather than creating an n cof vector, we're  going to create an n cof by one matrix.  So in math, we would probably call that a column vector.  But I think that's a kind of a dumb name in some ways because it's a matrix, right?   \n",
       "35                          And we don't just have inputs now.  We now also have weights, which are also called parameters.  And the key thing is this.  The model is not any more a bunch of conditionals  and loops and things.  It's a mathematical function.  In the case of a neural network, it's  a mathematical function that takes the inputs,  multiplies them together by the weights, by one set of weights,  and adds them up.  And then it does that again for a second set of weights  and adds them up.   \n",
       "34    So the size of this is called its kernel size. This is a three by three kernel for this convolution.  So then, because this is deep learning, we just repeat the, we just repeat these steps again and again and again.  So this is this layer I'm calling conv one, it's the first convolutional layer.  So con two, it's going to be a little bit different because on con one, we only had a single channel input. It's just black and white or, you know, yeah, black and white, grayscale one channel.  B...   \n",
       "33    What are these numbers and where it is that they come from and how come these numbers  can figure out whether something is a basset hound or not?  Okay.  So to answer that question, we're going to have a look at a Kaggle notebook.  How does a neural network really work?  I've got a local version of it here, which I'm going to take you through.  And the basic idea is machine learning models are things that fit functions to data.  So we start out with a very, very flexible, in fact, an infin...   \n",
       "32                                 The model is not any more a bunch of conditionals  and loops and things.  It's a mathematical function.  In the case of a neural network, it's  a mathematical function that takes the inputs,  multiplies them together by the weights, by one set of weights,  and adds them up.  And then it does that again for a second set of weights  and adds them up.  It does it again for a third set of weights and adds them up,  and so forth.  It then takes all the negative numbers   \n",
       "31    Right? You could have, we could just have easily done five by five, and then we'd have a five by five matrix of coefficients, or whatever, whatever size you like.  So the size of this is called its kernel size. This is a three by three kernel for this convolution.  So then, because this is deep learning, we just repeat the, we just repeat these steps again and again and again.  So this is this layer I'm calling conv one, it's the first convolutional layer.  So con two, it's going to be a l...   \n",
       "30    I could say, okay, we've got all these different things a BCD FTH J. Let's put them all into a single vector.  And then let's create a single matrix that has alpha alpha alpha alpha beta beta beta beta etc.  And then if we do this matrix multiplied by this vector, we get this with these gray zeros in the appropriate places, which gives us this, which is the same as this.  And so this shows that a convolution is actually a special kind of matrix modification. It's a matrix modification wher...   \n",
       "29    ahead and create a neural network.  So with a neural network, remember back to the Excel days, notice here is the same thing,  right?  A column vector, but we didn't create a column vector, we actually created a matrix with  kind of two sets of coefficients.  So when we did our matrix model play, every row gave us two sets of outputs, which we  then chucked through value, right, which remember we just used an if statement, and we added  them together.  So our cof's now, to make a proper ne...   \n",
       "28    But here's another way of thinking about it.  I could say, okay, we've got all these different things a BCD FTH J. Let's put them all into a single vector.  And then let's create a single matrix that has alpha alpha alpha alpha beta beta beta beta etc.  And then if we do this matrix multiplied by this vector, we get this with these gray zeros in the appropriate places, which gives us this, which is the same as this.  And so this shows that a convolution is actually a special kind of matrix...   \n",
       "27    But it's doing it this way where it's kind of taking advantage of the geometry of the situation that the things that are close to each other.  Being multiplied by this consistent group of the same nine weights each time.  Because there's actually 28 by 28 numbers here, right, which I think is 768.  That plus enough, 784.  But we don't want we don't we don't have 784 parameters really have nine parameters.  And so this is called a convolution.  So a convolution is where you basically slide ...   \n",
       "26    So initially, this thing doesn't do anything useful at all.  So what we do, the way Arthur Samuel described it back  in the late 50s, the inventor of machine learning,  is he said, OK, let's take the inputs and the weights,  put them through our model.  He wasn't talking particularly about neural networks.  He's just like, whatever model you like.  Get the results.  And then let's decide how good they are.  So if, for example, we're trying to decide,  is this a picture of a bird?  And the ...   \n",
       "25    dot F C one and parameters and other big bunch of numbers.  So what's going on here?  What are these numbers and where it is that they come from and how come these numbers  can figure out whether something is a basset hound or not?  Okay.  So to answer that question, we're going to have a look at a Kaggle notebook.  How does a neural network really work?  I've got a local version of it here, which I'm going to take you through.  And the basic idea is machine learning models are things that...   \n",
       "24    So what's going on here?  What are these numbers and where it is that they come from and how come these numbers  can figure out whether something is a basset hound or not?  Okay.  So to answer that question, we're going to have a look at a Kaggle notebook.  How does a neural network really work?  I've got a local version of it here, which I'm going to take you through.  And the basic idea is machine learning models are things that fit functions to data.  So we start out with a very, very f...   \n",
       "23    And they don't have to be three by three.  Right? You could have, we could just have easily done five by five, and then we'd have a five by five matrix of coefficients, or whatever, whatever size you like.  So the size of this is called its kernel size. This is a three by three kernel for this convolution.  So then, because this is deep learning, we just repeat the, we just repeat these steps again and again and again.  So this is this layer I'm calling conv one, it's the first convolution...   \n",
       "22    And layer one had sets of weights that found diagonal edges.  And here are some examples of bits of photos  that successfully matched with and opposite diagonal edges.  And kind of color gradients.  And here's some examples of bits of pictures that matched.  And then layer two combined, those.  And now you know how those were combined, right?  These were rectified linear units that were added together.  And then sets of those rectified linear units,  the outputs of those, they're called ac...   \n",
       "21                        As I say, it's very successful.  But it's not something that I could create for you  in a minute at the start of a course.  The difference with neural networks  is neural networks don't require us to build these features.  They build them for us.  And so what actually happened was, in I think it was 2015,  Matt Zyla and Rob Fergus took a trained neural network  and they looked inside it to see what it had learned.  So we don't give it features, we ask it to learn features.   \n",
       "20         called a model.  And we don't just have inputs now.  We now also have weights, which are also called parameters.  And the key thing is this.  The model is not any more a bunch of conditionals  and loops and things.  It's a mathematical function.  In the case of a neural network, it's  a mathematical function that takes the inputs,  multiplies them together by the weights, by one set of weights,  and adds them up.  And then it does that again for a second set of weights  and adds them up.   \n",
       "19    But it's not something that I could create for you  in a minute at the start of a course.  The difference with neural networks  is neural networks don't require us to build these features.  They build them for us.  And so what actually happened was, in I think it was 2015,  Matt Zyla and Rob Fergus took a trained neural network  and they looked inside it to see what it had learned.  So we don't give it features, we ask it to learn features.  So when Zyla and Fergus looked inside a neural n...   \n",
       "18    And the basic idea is machine learning models are things that fit functions to data.  So we start out with a very, very flexible, in fact, an infinitely flexible as we discussed  function, a neural network.  And we get it to do a particular thing, which is to recognize the patterns in the data examples  we give it.  So let's do a much simpler example than a neural network.  Let's do a quadratic.  So let's create a function F, which is 3x squared plus 2x plus 1.  Okay.  So it's a quadratic ...   \n",
       "17    And that's called a neural network.  Now, the model, therefore, is not  going to do anything useful.  And this leads weights to very carefully, it shows it.  And so the way it works is that we actually  start out with these weights as being random.  So initially, this thing doesn't do anything useful at all.  So what we do, the way Arthur Samuel described it back  in the late 50s, the inventor of machine learning,  is he said, OK, let's take the inputs and the weights,  put them through ou...   \n",
       "16    The foundations haven't changed. And it's not that different in fact to the convolutional neural network that Jan LeCun used on MNIST back in 1996.  It's, you know, the basic ideas I've described are forever, you know, the way the inputs work and the sandwiches of matrix,  and the model applies and activation functions and the stuff you do to the final layer, you know, everything else is tweaks.  And the more you learn about those basic ideas, the more you'll recognize those tweaks as simp...   \n",
       "15                                                           and adds them up.  And then it does that again for a second set of weights  and adds them up.  It does it again for a third set of weights and adds them up,  and so forth.  It then takes all the negative numbers  and replaces them with zeros.  And then it takes those as inputs to a next layer.  It does the same thing.  It multiplies them a bunch of times and adds them up.  And it does that a few times.  And that's called a neural network.   \n",
       "14    So with a neural network, remember back to the Excel days, notice here is the same thing,  right?  A column vector, but we didn't create a column vector, we actually created a matrix with  kind of two sets of coefficients.  So when we did our matrix model play, every row gave us two sets of outputs, which we  then chucked through value, right, which remember we just used an if statement, and we added  them together.  So our cof's now, to make a proper neural net, we need one set of cof's h...   \n",
       "13    So you can think of a convolution as being a sliding window of little mini dot products of these little three by three matrices.  And they don't have to be three by three.  Right? You could have, we could just have easily done five by five, and then we'd have a five by five matrix of coefficients, or whatever, whatever size you like.  So the size of this is called its kernel size. This is a three by three kernel for this convolution.  So then, because this is deep learning, we just repeat ...   \n",
       "12                                           And then it does that again for a second set of weights  and adds them up.  It does it again for a third set of weights and adds them up,  and so forth.  It then takes all the negative numbers  and replaces them with zeros.  And then it takes those as inputs to a next layer.  It does the same thing.  It multiplies them a bunch of times and adds them up.  And it does that a few times.  And that's called a neural network.  Now, the model, therefore, is not   \n",
       "11                                             And the key thing is this.  The model is not any more a bunch of conditionals  and loops and things.  It's a mathematical function.  In the case of a neural network, it's  a mathematical function that takes the inputs,  multiplies them together by the weights, by one set of weights,  and adds them up.  And then it does that again for a second set of weights  and adds them up.  It does it again for a third set of weights and adds them up,  and so forth.   \n",
       "10                        It's a mathematical function.  In the case of a neural network, it's  a mathematical function that takes the inputs,  multiplies them together by the weights, by one set of weights,  and adds them up.  And then it does that again for a second set of weights  and adds them up.  It does it again for a third set of weights and adds them up,  and so forth.  It then takes all the negative numbers  and replaces them with zeros.  And then it takes those as inputs to a next layer.   \n",
       "9     Okay.  So to answer that question, we're going to have a look at a Kaggle notebook.  How does a neural network really work?  I've got a local version of it here, which I'm going to take you through.  And the basic idea is machine learning models are things that fit functions to data.  So we start out with a very, very flexible, in fact, an infinitely flexible as we discussed  function, a neural network.  And we get it to do a particular thing, which is to recognize the patterns in the data...   \n",
       "8     I've got a local version of it here, which I'm going to take you through.  And the basic idea is machine learning models are things that fit functions to data.  So we start out with a very, very flexible, in fact, an infinitely flexible as we discussed  function, a neural network.  And we get it to do a particular thing, which is to recognize the patterns in the data examples  we give it.  So let's do a much simpler example than a neural network.  Let's do a quadratic.  So let's create a f...   \n",
       "7     and loops and things.  It's a mathematical function.  In the case of a neural network, it's  a mathematical function that takes the inputs,  multiplies them together by the weights, by one set of weights,  and adds them up.  And then it does that again for a second set of weights  and adds them up.  It does it again for a third set of weights and adds them up,  and so forth.  It then takes all the negative numbers  and replaces them with zeros.  And then it takes those as inputs to a next ...   \n",
       "6      So to answer that question, we're going to have a look at a Kaggle notebook.  How does a neural network really work?  I've got a local version of it here, which I'm going to take you through.  And the basic idea is machine learning models are things that fit functions to data.  So we start out with a very, very flexible, in fact, an infinitely flexible as we discussed  function, a neural network.  And we get it to do a particular thing, which is to recognize the patterns in the data examples   \n",
       "5            How does a neural network really work?  I've got a local version of it here, which I'm going to take you through.  And the basic idea is machine learning models are things that fit functions to data.  So we start out with a very, very flexible, in fact, an infinitely flexible as we discussed  function, a neural network.  And we get it to do a particular thing, which is to recognize the patterns in the data examples  we give it.  So let's do a much simpler example than a neural network.   \n",
       "4                               In the case of a neural network, it's  a mathematical function that takes the inputs,  multiplies them together by the weights, by one set of weights,  and adds them up.  And then it does that again for a second set of weights  and adds them up.  It does it again for a third set of weights and adds them up,  and so forth.  It then takes all the negative numbers  and replaces them with zeros.  And then it takes those as inputs to a next layer.  It does the same thing.   \n",
       "3     So I thought we might look at one of the most important and interesting version of things that can go in the middle.  But what you'll see is it turns out it's actually just another kind of matrix modification, which might not be obvious at first, but I'll explain.  We're going to look at something called a convolution, and convolutions are at the heart of a convolutional neural network.  So the first thing to realize is a convolutional neural network is very, very, very similar to the neur...   \n",
       "2     But what you'll see is it turns out it's actually just another kind of matrix modification, which might not be obvious at first, but I'll explain.  We're going to look at something called a convolution, and convolutions are at the heart of a convolutional neural network.  So the first thing to realize is a convolutional neural network is very, very, very similar to the neural networks we've seen so far. It's got imports. It's got things that are a lot like are actually are a form of matrix...   \n",
       "1     So the first thing to realize is a convolutional neural network is very, very, very similar to the neural networks we've seen so far. It's got imports. It's got things that are a lot like are actually are a form of matrix multiplication sandwich with activation functions, which can be rectified linear.  But there's a particular thing, which makes them very useful for computer vision. And I'm going to show you using this Excel spreadsheet that's in our repo called conv example.  And we're g...   \n",
       "0     We're going to look at something called a convolution, and convolutions are at the heart of a convolutional neural network.  So the first thing to realize is a convolutional neural network is very, very, very similar to the neural networks we've seen so far. It's got imports. It's got things that are a lot like are actually are a form of matrix multiplication sandwich with activation functions, which can be rectified linear.  But there's a particular thing, which makes them very useful for...   \n",
       "\n",
       "       score  \n",
       "49  0.566333  \n",
       "48  0.566635  \n",
       "47  0.566640  \n",
       "46  0.566887  \n",
       "45  0.568176  \n",
       "44  0.568182  \n",
       "43  0.568650  \n",
       "42  0.569121  \n",
       "41  0.569592  \n",
       "40  0.570379  \n",
       "39  0.570419  \n",
       "38  0.572909  \n",
       "37  0.575390  \n",
       "36  0.576456  \n",
       "35  0.577518  \n",
       "34  0.578256  \n",
       "33  0.578541  \n",
       "32  0.578821  \n",
       "31  0.579774  \n",
       "30  0.580621  \n",
       "29  0.580956  \n",
       "28  0.583673  \n",
       "27  0.585658  \n",
       "26  0.586377  \n",
       "25  0.588155  \n",
       "24  0.588817  \n",
       "23  0.590231  \n",
       "22  0.590644  \n",
       "21  0.590785  \n",
       "20  0.591769  \n",
       "19  0.592611  \n",
       "18  0.594546  \n",
       "17  0.598521  \n",
       "16  0.601152  \n",
       "15  0.601690  \n",
       "14  0.605886  \n",
       "13  0.610260  \n",
       "12  0.613787  \n",
       "11  0.614021  \n",
       "10  0.614602  \n",
       "9   0.622364  \n",
       "8   0.623059  \n",
       "7   0.628380  \n",
       "6   0.637606  \n",
       "5   0.638342  \n",
       "4   0.639722  \n",
       "3   0.654447  \n",
       "2   0.710898  \n",
       "1   0.713412  \n",
       "0   0.741815  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(embeddings.search('what is a convolutional neural network?',limit=50)).sort_values('score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a467376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.youtube.com/watch?v=htiNBPxcXgo&t=2757s'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytt['url'][8413]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dba81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051c1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.youtube.com/watch?v=8SF_h3xF3cE&list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU&index=1&t=1713s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b0476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://www.youtube.com/watch?v=8SF_h3xF3cE&t=1713s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b8e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "retriever = SentenceTransformer('flax-sentence-embeddings/all_datasets_v3_mpnet-base')\n",
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ff222",
   "metadata": {},
   "source": [
    "We can see the embedding dimension of `768` above, we will need this when creating our Pinecone index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa3db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = retriever.get_sentence_embedding_dimension()\n",
    "embed_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcea76e1",
   "metadata": {},
   "source": [
    "Now we can initialize our index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c936e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.01,\n",
       " 'namespaces': {'': {'vector_count': 11298}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f712a",
   "metadata": {},
   "source": [
    "# Querying\n",
    "\n",
    "When query we encode our text with the same retriever model and pass it to the Pinecone `query` endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fe732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15668c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is deep learning?\"\n",
    "\n",
    "xq = retriever.encode([query]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98c5714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " terms of optimization but what's the algorithm for updating the parameters or updating whatever the state of the network is and then the the last part is the the data set like how do you actually represent the world as it comes into your machine learning system so I think of deep learning as telling us something about what does the model look like and basically to qualify as deep I\n",
      "---\n",
      " any theoretical components any theoretical things that you need to understand about deep learning can be sick later for that link again just watched the word doc file again in that I mentioned the link also the second channel is my channel because deep learning might be complete deep learning playlist that I have created is completely in order okay to the other\n",
      "---\n",
      " under a rock for the last few years you have heard of the deep networks and how they have revolutionised computer vision and kind of the standard classic way of doing this is it's basically a classic supervised learning problem you are giving a network which you can think of as a big black box a pairs of input images and output labels XY pairs okay and this big black box essentially you\n",
      "---\n",
      " do the task at hand. Now deep learning is just a subset of machine learning which takes this idea even a step further and says how can we automatically extract the useful pieces of information needed to inform those future predictions or make a decision And that's what this class is all about teaching algorithms how to learn a task directly from raw data. We want to\n",
      "---\n",
      " algorithm and yelled at everybody in a good way that nobody was answering it correctly everybody knew what the alkyl it was graduate course everybody knew what an algorithm was but they weren't able to answer it well let me ask you in that same spirit what is deep learning I would say deep learning is any kind of machine learning that involves learning parameters of more than one consecutive\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "xc = index.query(xq, top_k=5,\n",
    "                 include_metadata=True)\n",
    "for context in xc['results'][0]['matches']:\n",
    "    print(context['metadata']['text'], end=\"\\n---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
